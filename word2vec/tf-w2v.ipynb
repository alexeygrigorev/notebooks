{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "from collections import Counter, deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Implementation on Tensor Flow\n",
    "\n",
    "- Data from https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "- Code from https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/home/agrigorev/tmp/data/bagofpopcorn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_folder + '/labeledTrainData.tsv', delimiter=\"\\t\", quoting=3)\n",
    "train_unlab = pd.read_csv(data_folder + '/unlabeledTrainData.tsv', delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(data_folder + '/testData.tsv', delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=True):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "   \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def clean_review(review, remove_stopwords=True):\n",
    "    return ' '.join(review_to_wordlist(review, remove_stopwords))\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) == 0:\n",
    "            continue\n",
    "        sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer()\n",
    "stokenizer = partial(review_to_sentences, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigorev/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/agrigorev/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "train['review_clean'] = train.review.apply(stokenizer)\n",
    "train_unlab['review_clean'] = train.review.apply(stokenizer)\n",
    "test['review_clean'] = test.review.apply(stokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for ss in train.review_clean:\n",
    "    if isinstance(ss, list):\n",
    "        sentences.extend(ss)\n",
    "\n",
    "for ss in train_unlab.review_clean:\n",
    "    if isinstance(ss, list):\n",
    "        sentences.extend(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(sentences, min_word_count):\n",
    "    count = Counter()\n",
    "    for s in sentences:\n",
    "        count.update(s)\n",
    "\n",
    "    UNK = 0\n",
    "    dictionary = {u'UNK': UNK}\n",
    "\n",
    "    for idx, (w, c) in enumerate(count.most_common(), start=1):\n",
    "        if c < min_word_count:\n",
    "            break\n",
    "        dictionary[w] = idx\n",
    "\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        data.append([dictionary.get(w, 0) for w in s])\n",
    "    \n",
    "    reverse_dict = {idx: w for (w, idx) in dictionary.items()}\n",
    "    return data, count, dictionary, reverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_word_count = 30\n",
    "data, count, dictionary, reverse_dict = build_dataset(sentences, min_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/3190706/nonlocal-keyword-in-python-2-x\n",
    "class Namespace(object): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, num_skips, skip_window, start_sentence=0):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "\n",
    "    window_center = skip_window\n",
    "    span = 2 * skip_window + 1\n",
    "\n",
    "    len_data = len(data)\n",
    "\n",
    "    ns = Namespace()\n",
    "    ns.current_sentence = start_sentence\n",
    "\n",
    "    def overlapping_chunks(n):\n",
    "        while 1:\n",
    "            alist = data[ns.current_sentence]\n",
    "            list_len = len(alist)\n",
    "            for i in xrange(0, list_len-n+1):\n",
    "                yield alist[i:i+n]\n",
    "            ns.current_sentence = (ns.current_sentence + 1) % len_data\n",
    "\n",
    "    def positive_samples(chunks):\n",
    "        for window in chunks:\n",
    "            target = window[window_center]\n",
    "            for i, context in enumerate(window):\n",
    "                if i == window_center:\n",
    "                    continue\n",
    "                yield [target, context]\n",
    "\n",
    "    chunk_gen = overlapping_chunks(n=span)\n",
    "    data_gen = positive_samples(chunk_gen)\n",
    "\n",
    "    while 1:\n",
    "        target_context = list(itertools.islice(data_gen, batch_size))\n",
    "        target_context = np.array(target_context, dtype=np.int32)\n",
    "\n",
    "        target = target_context[:, 0]\n",
    "        context = target_context[:, 1].reshape(-1, 1)\n",
    "\n",
    "        yield target, context, ns.current_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15558"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(reverse_dict)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # input data\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # variables\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # model\n",
    "\n",
    "    # look up embeddings for inputs\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                                                     train_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 6.0981 (sentence 4)\n",
      "Nearest to love        : mayhem, diagnosed, upper, activists, schneider, completist, genetically, heist\n",
      "Nearest to many        : depressing, tend, trey, stepmother, knife, punish, prettier, nails\n",
      "Nearest to end         : blocks, outrageous, uttered, idiots, mattered, silently, privilege, lifting\n",
      "Nearest to years       : town, curtis, novelty, volunteers, pleasing, discovering, paralyzed, shaved\n",
      "Nearest to think       : hostel, isabelle, plotline, distributors, kicking, spawned, competitors, abbot\n",
      "Nearest to something   : personas, roosevelt, hellman, lance, misfire, exhibition, ouch, resident\n",
      "Nearest to one         : bel, appropriately, ignores, elegant, bone, wears, baker, project\n",
      "Nearest to acting      : doggie, bickering, bike, struggle, colonies, suburban, glib, babble\n",
      "Nearest to UNK         : cringed, object, denholm, welfare, kitamura, flower, freddy, crothers\n",
      "Nearest to way         : morbius, introspection, forced, dumps, depicting, levy, pole, norris\n",
      "Nearest to scene       : flare, fuss, fever, invincible, disgraceful, lori, magnus, mysticism\n",
      "Nearest to still       : largest, bean, rightly, mime, complement, narcissism, turmoil, nic\n",
      "Nearest to didn        : nine, virtue, morty, guitar, doorstep, outline, access, layer\n",
      "Nearest to another     : favours, bet, delivering, africa, seem, litter, sufficiently, tie\n",
      "Nearest to got         : flea, appears, crooked, steele, worthless, embarrassment, piscopo, laurel\n",
      "Nearest to seems       : yarn, non, attendance, outlaw, holding, fellowship, goose, relates\n",
      "Average loss at step 2000: 4.5698 (sentence 13330)\n",
      "Average loss at step 4000: 4.2237 (sentence 27057)\n",
      "Average loss at step 6000: 4.1211 (sentence 40411)\n",
      "Average loss at step 8000: 4.0753 (sentence 54208)\n",
      "Average loss at step 10000: 4.0478 (sentence 68000)\n",
      "Average loss at step 12000: 4.0035 (sentence 81781)\n",
      "Average loss at step 14000: 3.9801 (sentence 95344)\n",
      "Average loss at step 16000: 3.9601 (sentence 109106)\n",
      "Average loss at step 18000: 3.9423 (sentence 122759)\n",
      "Average loss at step 20000: 3.9347 (sentence 136724)\n",
      "Average loss at step 22000: 3.9154 (sentence 150265)\n",
      "Average loss at step 24000: 3.8987 (sentence 164061)\n",
      "Average loss at step 26000: 3.8875 (sentence 177723)\n",
      "Average loss at step 28000: 3.8755 (sentence 191303)\n",
      "Average loss at step 30000: 3.8646 (sentence 205043)\n",
      "Average loss at step 32000: 3.8487 (sentence 218538)\n",
      "Average loss at step 34000: 3.8458 (sentence 232155)\n",
      "Average loss at step 36000: 3.8340 (sentence 245827)\n",
      "Average loss at step 38000: 3.8237 (sentence 259776)\n",
      "Average loss at step 40000: 3.7821 (sentence 273577)\n",
      "Average loss at step 42000: 3.7220 (sentence 286926)\n",
      "Average loss at step 44000: 3.7129 (sentence 300770)\n",
      "Average loss at step 46000: 3.7068 (sentence 314114)\n",
      "Average loss at step 48000: 3.7124 (sentence 327942)\n",
      "Average loss at step 50000: 3.7020 (sentence 341685)\n",
      "Nearest to love        : relationship, literal, induced, true, sho, newsreel, norton, implications\n",
      "Nearest to many        : several, various, number, couple, usually, lot, two, chile\n",
      "Nearest to end         : final, ending, opening, started, anywhere, beginning, concerned, closing\n",
      "Nearest to years       : year, days, months, weeks, minutes, times, decades, hours\n",
      "Nearest to think       : thought, understand, believe, feel, know, say, probably, wish\n",
      "Nearest to something   : anything, nothing, someone, everything, things, much, matrix, operating\n",
      "Nearest to one         : perhaps, entry, week, every, mexican, coroner, newspapers, executing\n",
      "Nearest to acting      : editing, writing, script, directing, performances, direction, dialogue, job\n",
      "Nearest to UNK         : l, janice, pipe, p, london, lyle, alain, nerve\n",
      "Nearest to way         : personnel, things, forced, sleep, consistent, effort, far, bog\n",
      "Nearest to scene       : scenes, sequence, sequences, episode, moment, moments, thing, liners\n",
      "Nearest to still       : always, actually, marisa, extremely, piscopo, also, inmates, greedy\n",
      "Nearest to didn        : doesn, wanted, wouldn, couldn, ll, might, won, would\n",
      "Nearest to another     : second, every, anonymous, lifetime, hilton, entity, impossible, franchot\n",
      "Nearest to got         : gets, get, getting, heard, seen, carre, injured, bradley\n",
      "Nearest to seems       : seemed, feels, looks, seem, felt, sounds, feel, would\n",
      "Average loss at step 52000: 3.6868 (sentence 355371)\n",
      "Average loss at step 54000: 3.6969 (sentence 369050)\n",
      "Average loss at step 56000: 3.6812 (sentence 382921)\n",
      "Average loss at step 58000: 3.6942 (sentence 396314)\n",
      "Average loss at step 60000: 3.6897 (sentence 410482)\n",
      "Average loss at step 62000: 3.6804 (sentence 424075)\n",
      "Average loss at step 64000: 3.6719 (sentence 437673)\n",
      "Average loss at step 66000: 3.6729 (sentence 451462)\n",
      "Average loss at step 68000: 3.6666 (sentence 465134)\n",
      "Average loss at step 70000: 3.6600 (sentence 478698)\n",
      "Average loss at step 72000: 3.6629 (sentence 492149)\n",
      "Average loss at step 74000: 3.6614 (sentence 505655)\n",
      "Average loss at step 76000: 3.6528 (sentence 519681)\n",
      "Average loss at step 78000: 3.6507 (sentence 533599)\n",
      "Average loss at step 80000: 3.5950 (sentence 10352)\n",
      "Average loss at step 82000: 3.6029 (sentence 24021)\n",
      "Average loss at step 84000: 3.5850 (sentence 37463)\n",
      "Average loss at step 86000: 3.5806 (sentence 51190)\n",
      "Average loss at step 88000: 3.5901 (sentence 65005)\n",
      "Average loss at step 90000: 3.5747 (sentence 78683)\n",
      "Average loss at step 92000: 3.5708 (sentence 92255)\n",
      "Average loss at step 94000: 3.5773 (sentence 106031)\n",
      "Average loss at step 96000: 3.5720 (sentence 119726)\n",
      "Average loss at step 98000: 3.5797 (sentence 133614)\n",
      "Average loss at step 100000: 3.5788 (sentence 147329)\n",
      "Nearest to love        : loved, relationship, flavor, newsreel, true, induced, sho, hate\n",
      "Nearest to many        : several, various, countless, number, numerous, usually, couple, weren\n",
      "Nearest to end         : beginning, ended, final, ending, closing, middle, climax, started\n",
      "Nearest to years       : year, months, days, decade, weeks, decades, minutes, hours\n",
      "Nearest to think       : thought, believe, understand, wish, know, explain, hoped, standup\n",
      "Nearest to something   : anything, nothing, someone, everything, things, much, matrix, stuff\n",
      "Nearest to one         : single, perhaps, possibly, three, scrubs, satires, among, kipling\n",
      "Nearest to acting      : directing, writing, performances, script, editing, direction, performance, casting\n",
      "Nearest to UNK         : pipe, snippets, horny, morris, deol, yuk, burrows, crothers\n",
      "Nearest to way         : consistent, orthodox, ways, overplayed, sheffer, miraglia, conditioned, awkwardly\n",
      "Nearest to scene       : scenes, sequence, sequences, moments, moment, episode, thing, mencia\n",
      "Nearest to still       : always, actually, also, even, really, somehow, generally, definitely\n",
      "Nearest to didn        : doesn, wouldn, couldn, wanted, won, ll, might, begin\n",
      "Nearest to another     : every, second, anonymous, reels, lifetime, overwrought, gerry, pretension\n",
      "Nearest to got         : gets, get, getting, gotten, gave, heard, walked, challenger\n",
      "Nearest to seems       : seemed, feels, felt, seem, looks, sounds, appears, sort\n",
      "Average loss at step 102000: 3.5693 (sentence 160938)\n",
      "Average loss at step 104000: 3.5642 (sentence 174642)\n",
      "Average loss at step 106000: 3.5692 (sentence 188288)\n",
      "Average loss at step 108000: 3.5604 (sentence 201838)\n",
      "Average loss at step 110000: 3.5540 (sentence 215507)\n",
      "Average loss at step 112000: 3.5659 (sentence 229095)\n",
      "Average loss at step 114000: 3.5579 (sentence 242646)\n",
      "Average loss at step 116000: 3.5558 (sentence 256822)\n",
      "Average loss at step 118000: 3.5439 (sentence 270558)\n",
      "Average loss at step 120000: 3.5099 (sentence 284019)\n",
      "Average loss at step 122000: 3.5148 (sentence 297717)\n",
      "Average loss at step 124000: 3.5038 (sentence 310978)\n",
      "Average loss at step 126000: 3.5146 (sentence 324814)\n",
      "Average loss at step 128000: 3.5120 (sentence 338625)\n",
      "Average loss at step 130000: 3.4924 (sentence 352288)\n",
      "Average loss at step 132000: 3.5038 (sentence 366073)\n",
      "Average loss at step 134000: 3.4974 (sentence 379773)\n",
      "Average loss at step 136000: 3.5094 (sentence 393369)\n",
      "Average loss at step 138000: 3.5001 (sentence 407405)\n",
      "Average loss at step 140000: 3.5068 (sentence 421031)\n",
      "Average loss at step 142000: 3.4944 (sentence 434632)\n",
      "Average loss at step 144000: 3.4953 (sentence 448366)\n",
      "Average loss at step 146000: 3.4920 (sentence 461931)\n",
      "Average loss at step 148000: 3.4952 (sentence 475742)\n",
      "Average loss at step 150000: 3.4896 (sentence 489116)\n",
      "Nearest to love        : hate, asleep, relationship, loved, newsreel, essence, shared, sho\n",
      "Nearest to many        : several, countless, various, numerous, number, multiple, thousands, hundreds\n",
      "Nearest to end         : final, ended, ending, climax, ends, beginning, middle, closing\n",
      "Nearest to years       : months, year, days, weeks, decade, decades, minutes, times\n",
      "Nearest to think       : believe, thought, appreciate, sure, hodgepodge, hoped, wish, understand\n",
      "Nearest to something   : anything, nothing, someone, matrix, everything, cheh, goings, much\n",
      "Nearest to one         : troma, single, mesmerized, among, possibly, every, carell, december\n",
      "Nearest to acting      : writing, directing, script, performances, editing, dialogue, direction, delivery\n",
      "Nearest to UNK         : preach, yuk, alpha, cap, admits, pipe, overtly, chef\n",
      "Nearest to way         : ways, awkwardly, situations, cleese, overplayed, conditioned, stripes, boxing\n",
      "Nearest to scene       : sequence, scenes, sequences, moment, moments, thing, victim, episode\n",
      "Nearest to still       : always, somehow, actually, certainly, definitely, resounding, wicker, willingly\n",
      "Nearest to didn        : doesn, couldn, won, wanted, wouldn, ll, shouldn, would\n",
      "Nearest to another     : every, strangest, final, last, reels, gerry, overwrought, mayor\n",
      "Nearest to got         : gets, get, getting, gotten, gave, walked, finished, watched\n",
      "Nearest to seems       : seemed, seem, feels, looks, sounds, felt, appears, looked\n",
      "Average loss at step 152000: 3.4952 (sentence 502748)\n",
      "Average loss at step 154000: 3.4889 (sentence 516621)\n",
      "Average loss at step 156000: 3.4905 (sentence 530504)\n",
      "Average loss at step 158000: 3.4636 (sentence 7423)\n",
      "Average loss at step 160000: 3.4543 (sentence 20793)\n",
      "Average loss at step 162000: 3.4565 (sentence 34610)\n",
      "Average loss at step 164000: 3.4432 (sentence 47911)\n",
      "Average loss at step 166000: 3.4594 (sentence 61969)\n",
      "Average loss at step 168000: 3.4463 (sentence 75532)\n",
      "Average loss at step 170000: 3.4401 (sentence 89100)\n",
      "Average loss at step 172000: 3.4486 (sentence 103155)\n",
      "Average loss at step 174000: 3.4385 (sentence 116737)\n",
      "Average loss at step 176000: 3.4577 (sentence 130271)\n",
      "Average loss at step 178000: 3.4519 (sentence 144325)\n",
      "Average loss at step 180000: 3.4467 (sentence 157852)\n",
      "Average loss at step 182000: 3.4440 (sentence 171521)\n",
      "Average loss at step 184000: 3.4448 (sentence 185316)\n",
      "Average loss at step 186000: 3.4383 (sentence 198870)\n",
      "Average loss at step 188000: 3.4366 (sentence 212407)\n",
      "Average loss at step 190000: 3.4447 (sentence 226089)\n",
      "Average loss at step 192000: 3.4425 (sentence 239696)\n",
      "Average loss at step 194000: 3.4383 (sentence 253760)\n",
      "Average loss at step 196000: 3.4394 (sentence 267565)\n",
      "Average loss at step 198000: 3.4001 (sentence 281012)\n",
      "Average loss at step 200000: 3.4211 (sentence 294721)\n",
      "Nearest to love        : hate, lust, loved, asleep, shared, norton, relationship, enchanted\n",
      "Nearest to many        : several, countless, numerous, couple, various, multiple, usually, dozen\n",
      "Nearest to end         : climax, final, beginning, ended, ending, closing, opening, start\n",
      "Nearest to years       : months, year, days, weeks, decades, decade, month, minutes\n",
      "Nearest to think       : hoped, thinking, believe, wish, thought, say, understand, imagine\n",
      "Nearest to something   : anything, nothing, goings, sides, someone, stuff, aja, cheh\n",
      "Nearest to one         : sophomoric, bird, satires, funniest, winding, every, sepia, enemy\n",
      "Nearest to acting      : directing, writing, performances, script, editing, actors, direction, casting\n",
      "Nearest to UNK         : janice, lamarr, denholm, merk, airline, flower, ricky, steaming\n",
      "Nearest to way         : ways, situations, sometimes, regrettably, dragged, wings, bog, trader\n",
      "Nearest to scene       : scenes, sequence, sequences, moments, moment, thing, victim, montage\n",
      "Nearest to still       : always, also, neither, somehow, certainly, generally, genuinely, greedy\n",
      "Nearest to didn        : doesn, won, couldn, wanted, wouldn, ll, shouldn, would\n",
      "Nearest to another     : strangest, every, pass, powerful, overwrought, last, vows, pretension\n",
      "Nearest to got         : gets, get, gotten, getting, gave, walked, received, must\n",
      "Nearest to seems       : seemed, feels, seem, appears, looks, felt, sounds, becomes\n",
      "Average loss at step 202000: 3.4033 (sentence 308076)\n",
      "Average loss at step 204000: 3.4097 (sentence 321832)\n",
      "Average loss at step 206000: 3.4129 (sentence 335617)\n",
      "Average loss at step 208000: 3.3981 (sentence 349420)\n",
      "Average loss at step 210000: 3.3961 (sentence 363029)\n",
      "Average loss at step 212000: 3.4098 (sentence 376721)\n",
      "Average loss at step 214000: 3.4007 (sentence 390354)\n",
      "Average loss at step 216000: 3.4088 (sentence 404279)\n",
      "Average loss at step 218000: 3.4095 (sentence 417905)\n",
      "Average loss at step 220000: 3.4047 (sentence 431613)\n",
      "Average loss at step 222000: 3.4045 (sentence 445328)\n",
      "Average loss at step 224000: 3.4005 (sentence 458930)\n",
      "Average loss at step 226000: 3.4008 (sentence 472621)\n",
      "Average loss at step 228000: 3.3923 (sentence 486192)\n",
      "Average loss at step 230000: 3.4095 (sentence 499810)\n",
      "Average loss at step 232000: 3.4009 (sentence 513389)\n",
      "Average loss at step 234000: 3.3983 (sentence 527431)\n",
      "Average loss at step 236000: 3.3855 (sentence 4401)\n",
      "Average loss at step 238000: 3.3750 (sentence 17834)\n",
      "Average loss at step 240000: 3.3745 (sentence 31593)\n",
      "Average loss at step 242000: 3.3651 (sentence 44988)\n",
      "Average loss at step 244000: 3.3766 (sentence 58788)\n",
      "Average loss at step 246000: 3.3739 (sentence 72485)\n",
      "Average loss at step 248000: 3.3523 (sentence 86207)\n",
      "Average loss at step 250000: 3.3725 (sentence 99831)\n",
      "Nearest to love        : loved, hate, mcadams, lust, prey, relationship, asleep, liked\n",
      "Nearest to many        : several, countless, various, numerous, dozens, hundreds, thousands, couple\n",
      "Nearest to end         : climax, final, beginning, ending, climatic, conclusion, winds, ends\n",
      "Nearest to years       : months, year, days, decades, decade, weeks, minutes, month\n",
      "Nearest to think       : believe, wish, hoped, imagine, feel, understand, thinking, liked\n",
      "Nearest to something   : anything, nothing, goings, someone, everything, stuff, things, claptrap\n",
      "Nearest to one         : perhaps, sophomoric, entry, cautionary, twin, scrubs, rosanna, single\n",
      "Nearest to acting      : writing, directing, editing, scripts, script, direction, performances, casting\n",
      "Nearest to UNK         : commitments, rom, mccarthy, cumming, hicks, including, airline, relatable\n",
      "Nearest to way         : ways, wings, cleese, marlon, tremendously, conversations, denominator, crouse\n",
      "Nearest to scene       : scenes, sequence, moment, sequences, victim, montage, thing, moments\n",
      "Nearest to still       : also, always, certainly, definitely, actually, neither, even, generally\n",
      "Nearest to didn        : doesn, couldn, wouldn, won, wanted, ll, would, shouldn\n",
      "Nearest to another     : every, calamai, collectors, anonymous, pitiful, pretension, strangest, poisonous\n",
      "Nearest to got         : gets, get, gotten, getting, gave, received, walked, deserves\n",
      "Nearest to seems       : seemed, seem, feels, appears, felt, seemingly, looks, would\n",
      "Average loss at step 252000: 3.3654 (sentence 113730)\n",
      "Average loss at step 254000: 3.3834 (sentence 127265)\n",
      "Average loss at step 256000: 3.3703 (sentence 141349)\n",
      "Average loss at step 258000: 3.3748 (sentence 154956)\n",
      "Average loss at step 260000: 3.3631 (sentence 168533)\n",
      "Average loss at step 262000: 3.3704 (sentence 182333)\n",
      "Average loss at step 264000: 3.3693 (sentence 195946)\n",
      "Average loss at step 266000: 3.3630 (sentence 209503)\n",
      "Average loss at step 268000: 3.3660 (sentence 222973)\n",
      "Average loss at step 270000: 3.3726 (sentence 236548)\n",
      "Average loss at step 272000: 3.3641 (sentence 250590)\n",
      "Average loss at step 274000: 3.3666 (sentence 264347)\n",
      "Average loss at step 276000: 3.3331 (sentence 277970)\n",
      "Average loss at step 278000: 3.3552 (sentence 291565)\n",
      "Average loss at step 280000: 3.3435 (sentence 305121)\n",
      "Average loss at step 282000: 3.3331 (sentence 318766)\n",
      "Average loss at step 284000: 3.3499 (sentence 332644)\n",
      "Average loss at step 286000: 3.3351 (sentence 346302)\n",
      "Average loss at step 288000: 3.3355 (sentence 359912)\n",
      "Average loss at step 290000: 3.3452 (sentence 373724)\n",
      "Average loss at step 292000: 3.3366 (sentence 387331)\n",
      "Average loss at step 294000: 3.3487 (sentence 401286)\n",
      "Average loss at step 296000: 3.3464 (sentence 414934)\n",
      "Average loss at step 298000: 3.3406 (sentence 428594)\n",
      "Average loss at step 300000: 3.3436 (sentence 442293)\n",
      "Nearest to love        : loved, hate, lust, exploitive, mcadams, liked, asleep, appreciation\n",
      "Nearest to many        : several, countless, thousands, various, numerous, multiple, dozen, dozens\n",
      "Nearest to end         : ending, final, climax, tiring, ended, beginning, ends, winds\n",
      "Nearest to years       : year, months, days, decades, decade, weeks, month, minutes\n",
      "Nearest to think       : thought, hoped, consider, thinking, wonder, know, forgive, feel\n",
      "Nearest to something   : anything, nothing, everything, someone, awakening, stuff, much, goings\n",
      "Nearest to one         : cranked, single, possibly, three, among, troma, satires, entry\n",
      "Nearest to acting      : directing, performances, writing, editing, direction, dialogue, clunky, script\n",
      "Nearest to UNK         : janice, relatable, presidential, finch, marco, engle, plentiful, filter\n",
      "Nearest to way         : ways, shapiro, progressively, bicycle, conditioned, dragged, tremendously, conversations\n",
      "Nearest to scene       : scenes, sequence, moment, moments, montage, sequences, monologue, thing\n",
      "Nearest to still       : always, generally, neither, really, also, definitely, actually, somehow\n",
      "Nearest to didn        : doesn, wouldn, couldn, would, won, wanted, might, ll\n",
      "Nearest to another     : every, final, last, sheet, basically, ah, simplest, hartnett\n",
      "Nearest to got         : gets, gotten, get, getting, gave, received, rented, walked\n",
      "Nearest to seems       : seemed, seem, looks, feels, appears, sounds, felt, becomes\n",
      "Average loss at step 302000: 3.3390 (sentence 456007)\n",
      "Average loss at step 304000: 3.3368 (sentence 469482)\n",
      "Average loss at step 306000: 3.3300 (sentence 483172)\n",
      "Average loss at step 308000: 3.3490 (sentence 496740)\n",
      "Average loss at step 310000: 3.3428 (sentence 510208)\n",
      "Average loss at step 312000: 3.3327 (sentence 524504)\n",
      "Average loss at step 314000: 3.3355 (sentence 1506)\n",
      "Average loss at step 316000: 3.3054 (sentence 14898)\n",
      "Average loss at step 318000: 3.3221 (sentence 28569)\n",
      "Average loss at step 320000: 3.3133 (sentence 41844)\n",
      "Average loss at step 322000: 3.3178 (sentence 55633)\n",
      "Average loss at step 324000: 3.3299 (sentence 69528)\n",
      "Average loss at step 326000: 3.2982 (sentence 83232)\n",
      "Average loss at step 328000: 3.3163 (sentence 96900)\n",
      "Average loss at step 330000: 3.3124 (sentence 110653)\n",
      "Average loss at step 332000: 3.3268 (sentence 124211)\n",
      "Average loss at step 334000: 3.3206 (sentence 138268)\n",
      "Average loss at step 336000: 3.3185 (sentence 151842)\n",
      "Average loss at step 338000: 3.3160 (sentence 165583)\n",
      "Average loss at step 340000: 3.3145 (sentence 179232)\n",
      "Average loss at step 342000: 3.3154 (sentence 192785)\n",
      "Average loss at step 344000: 3.3100 (sentence 206605)\n",
      "Average loss at step 346000: 3.3098 (sentence 220005)\n",
      "Average loss at step 348000: 3.3229 (sentence 233595)\n",
      "Average loss at step 350000: 3.3136 (sentence 247505)\n",
      "Nearest to love        : hate, lust, loved, adore, asleep, relationship, betrayal, grace\n",
      "Nearest to many        : several, countless, thousands, numerous, various, multiple, majority, dozens\n",
      "Nearest to end         : final, climax, beginning, ending, winds, ended, climatic, closing\n",
      "Nearest to years       : year, months, days, decades, weeks, decade, month, minutes\n",
      "Nearest to think       : wonder, wish, thinking, feel, thought, hoped, praises, understand\n",
      "Nearest to something   : anything, nothing, someone, much, goings, simplistic, cheh, cautionary\n",
      "Nearest to one         : every, ultimatum, winding, cautionary, single, possibly, among, december\n",
      "Nearest to acting      : directing, writing, performances, script, editing, delivery, direction, embarrassingly\n",
      "Nearest to UNK         : panama, janice, loudly, raising, burgade, kingsley, byrne, burroughs\n",
      "Nearest to way         : ways, manner, progressively, bicycle, bog, side, reliance, line\n",
      "Nearest to scene       : sequence, scenes, sequences, moment, montage, victim, occurs, oral\n",
      "Nearest to still       : generally, always, actually, somehow, also, hunger, definately, manhood\n",
      "Nearest to didn        : doesn, couldn, won, wouldn, wanted, ll, didnt, would\n",
      "Nearest to another     : every, last, final, first, latest, previous, mcgovern, sheet\n",
      "Nearest to got         : gets, get, getting, gotten, gave, received, grabbed, walked\n",
      "Nearest to seems       : seemed, seem, feels, appears, looks, sounds, felt, looked\n",
      "Average loss at step 352000: 3.3177 (sentence 261290)\n",
      "Average loss at step 354000: 3.2910 (sentence 275048)\n",
      "Average loss at step 356000: 3.2968 (sentence 288375)\n",
      "Average loss at step 358000: 3.2980 (sentence 302272)\n",
      "Average loss at step 360000: 3.2892 (sentence 315537)\n",
      "Average loss at step 362000: 3.2968 (sentence 329547)\n",
      "Average loss at step 364000: 3.2917 (sentence 343174)\n",
      "Average loss at step 366000: 3.2819 (sentence 356737)\n",
      "Average loss at step 368000: 3.2986 (sentence 370647)\n",
      "Average loss at step 370000: 3.2885 (sentence 384417)\n",
      "Average loss at step 372000: 3.3047 (sentence 397831)\n",
      "Average loss at step 374000: 3.2940 (sentence 411973)\n",
      "Average loss at step 376000: 3.2976 (sentence 425494)\n",
      "Average loss at step 378000: 3.2963 (sentence 439172)\n",
      "Average loss at step 380000: 3.3000 (sentence 452960)\n",
      "Average loss at step 382000: 3.2859 (sentence 466565)\n",
      "Average loss at step 384000: 3.2869 (sentence 480105)\n",
      "Average loss at step 386000: 3.2983 (sentence 493735)\n",
      "Average loss at step 388000: 3.3007 (sentence 507281)\n",
      "Average loss at step 390000: 3.2857 (sentence 521409)\n",
      "Average loss at step 392000: 3.2958 (sentence 535180)\n",
      "Average loss at step 394000: 3.2645 (sentence 11890)\n",
      "Average loss at step 396000: 3.2860 (sentence 25523)\n",
      "Average loss at step 398000: 3.2687 (sentence 38963)\n",
      "Average loss at step 400000: 3.2757 (sentence 52621)\n",
      "Nearest to love        : betrayal, hate, loved, lust, captivated, true, mcadams, adore\n",
      "Nearest to many        : several, countless, numerous, thousands, various, multiple, dozens, dozen\n",
      "Nearest to end         : beginning, ending, final, climax, winds, tiring, disappointed, closing\n",
      "Nearest to years       : months, year, days, decade, decades, weeks, month, minutes\n",
      "Nearest to think       : hoped, wish, understand, thinking, believe, wonder, thought, guess\n",
      "Nearest to something   : anything, nothing, goings, cautionary, simplistic, stuff, fascinated, things\n",
      "Nearest to one         : ultimatum, theft, cautionary, perhaps, winding, damn, bird, winfrey\n",
      "Nearest to acting      : directing, performances, writing, script, clunky, dialogue, scripts, embarrassingly\n",
      "Nearest to UNK         : monarch, crothers, macgregor, droning, costumed, stephens, caribbean, caan\n",
      "Nearest to way         : ways, manner, attempt, wings, conversations, fascism, peculiar, side\n",
      "Nearest to scene       : scenes, sequence, sequences, moment, victim, hallucination, crow, thing\n",
      "Nearest to still       : also, always, actually, generally, somehow, sure, genuinely, hunger\n",
      "Nearest to didn        : doesn, couldn, won, wouldn, wanted, would, ll, need\n",
      "Nearest to another     : every, last, sheet, milk, vows, olympic, eternity, mcgovern\n",
      "Nearest to got         : gets, get, gotten, getting, gave, received, broke, sold\n",
      "Nearest to seems       : seemed, seem, feels, appears, felt, seemingly, looks, becomes\n",
      "Average loss at step 402000: 3.2826 (sentence 66493)\n",
      "Average loss at step 404000: 3.2635 (sentence 80252)\n",
      "Average loss at step 406000: 3.2664 (sentence 93870)\n",
      "Average loss at step 408000: 3.2807 (sentence 107583)\n",
      "Average loss at step 410000: 3.2701 (sentence 121266)\n",
      "Average loss at step 412000: 3.2809 (sentence 135146)\n",
      "Average loss at step 414000: 3.2795 (sentence 148798)\n",
      "Average loss at step 416000: 3.2748 (sentence 162441)\n",
      "Average loss at step 418000: 3.2783 (sentence 176211)\n",
      "Average loss at step 420000: 3.2745 (sentence 189781)\n",
      "Average loss at step 422000: 3.2724 (sentence 203473)\n",
      "Average loss at step 424000: 3.2664 (sentence 217098)\n",
      "Average loss at step 426000: 3.2820 (sentence 230657)\n",
      "Average loss at step 428000: 3.2807 (sentence 244187)\n",
      "Average loss at step 430000: 3.2714 (sentence 258226)\n",
      "Average loss at step 432000: 3.2654 (sentence 272057)\n",
      "Average loss at step 434000: 3.2519 (sentence 285523)\n",
      "Average loss at step 436000: 3.2647 (sentence 299151)\n",
      "Average loss at step 438000: 3.2478 (sentence 312618)\n",
      "Average loss at step 440000: 3.2607 (sentence 326301)\n",
      "Average loss at step 442000: 3.2660 (sentence 340073)\n",
      "Average loss at step 444000: 3.2364 (sentence 353811)\n",
      "Average loss at step 446000: 3.2605 (sentence 367494)\n",
      "Average loss at step 448000: 3.2592 (sentence 381330)\n",
      "Average loss at step 450000: 3.2655 (sentence 394857)\n",
      "Nearest to love        : hate, loved, liked, enjoy, loves, mcadams, lust, captivated\n",
      "Nearest to many        : several, countless, thousands, various, dozen, numerous, majority, dozens\n",
      "Nearest to end         : beginning, ending, ended, final, climax, winds, climatic, ends\n",
      "Nearest to years       : months, year, days, decade, decades, weeks, month, minutes\n",
      "Nearest to think       : thinking, feel, understand, believe, dislike, hoped, know, guess\n",
      "Nearest to something   : anything, nothing, things, everything, stuff, someone, goings, ang\n",
      "Nearest to one         : cautionary, perhaps, scrubs, among, single, every, possibly, winding\n",
      "Nearest to acting      : writing, directing, performances, dialogue, editing, embarrassingly, clunky, script\n",
      "Nearest to UNK         : panama, played, engle, pidgeon, custer, relatable, cumming, whips\n",
      "Nearest to way         : ways, side, danni, wings, cleese, outrageously, manner, silverstone\n",
      "Nearest to scene       : scenes, sequence, sequences, moment, moments, thing, enters, crow\n",
      "Nearest to still       : always, also, generally, actually, certainly, somehow, definitely, even\n",
      "Nearest to didn        : doesn, wouldn, couldn, won, would, wanted, ll, didnt\n",
      "Nearest to another     : every, yarn, mcgovern, last, ah, violin, sheet, electrocuted\n",
      "Nearest to got         : gets, get, gotten, getting, gave, received, walked, broke\n",
      "Nearest to seems       : seemed, appears, seem, feels, felt, looks, seemingly, sounding\n",
      "Average loss at step 452000: 3.2613 (sentence 409027)\n",
      "Average loss at step 454000: 3.2649 (sentence 422530)\n",
      "Average loss at step 456000: 3.2596 (sentence 436196)\n",
      "Average loss at step 458000: 3.2599 (sentence 449873)\n",
      "Average loss at step 460000: 3.2585 (sentence 463559)\n",
      "Average loss at step 462000: 3.2556 (sentence 477138)\n",
      "Average loss at step 464000: 3.2544 (sentence 490612)\n",
      "Average loss at step 466000: 3.2657 (sentence 504201)\n",
      "Average loss at step 468000: 3.2560 (sentence 518122)\n",
      "Average loss at step 470000: 3.2629 (sentence 532057)\n",
      "Average loss at step 472000: 3.2314 (sentence 8894)\n",
      "Average loss at step 474000: 3.2509 (sentence 22446)\n",
      "Average loss at step 476000: 3.2435 (sentence 36012)\n",
      "Average loss at step 478000: 3.2337 (sentence 49674)\n",
      "Average loss at step 480000: 3.2486 (sentence 63469)\n",
      "Average loss at step 482000: 3.2389 (sentence 77062)\n",
      "Average loss at step 484000: 3.2320 (sentence 90722)\n",
      "Average loss at step 486000: 3.2516 (sentence 104554)\n",
      "Average loss at step 488000: 3.2306 (sentence 118113)\n",
      "Average loss at step 490000: 3.2546 (sentence 132098)\n",
      "Average loss at step 492000: 3.2486 (sentence 145795)\n",
      "Average loss at step 494000: 3.2493 (sentence 159381)\n",
      "Average loss at step 496000: 3.2458 (sentence 173087)\n",
      "Average loss at step 498000: 3.2398 (sentence 186864)\n",
      "Average loss at step 500000: 3.2380 (sentence 200375)\n",
      "Nearest to love        : lust, loved, hate, captivated, pinjar, betrayal, flamenco, adore\n",
      "Nearest to many        : several, countless, thousands, various, numerous, multiple, dozen, majority\n",
      "Nearest to end         : final, ending, beginning, climax, tiring, ends, winds, climatic\n",
      "Nearest to years       : year, months, days, decades, decade, weeks, month, times\n",
      "Nearest to think       : thinking, believe, understand, thought, wonder, wish, consider, automatically\n",
      "Nearest to something   : anything, nothing, someone, everything, things, stuff, simplistic, claptrap\n",
      "Nearest to one         : single, cautionary, satires, sophomoric, four, three, cranked, contender\n",
      "Nearest to acting      : directing, performances, writing, delivery, direction, clunky, unremarkable, editing\n",
      "Nearest to UNK         : sade, engineering, janice, burroughs, panama, macgregor, fischer, however\n",
      "Nearest to way         : ways, progressively, conversations, silverstone, manner, shapiro, sing, bicycle\n",
      "Nearest to scene       : sequence, scenes, sequences, moment, moments, monologue, minute, bike\n",
      "Nearest to still       : always, actually, also, definitely, generally, somehow, hunger, certainly\n",
      "Nearest to didn        : doesn, wouldn, couldn, won, didnt, wanted, would, cannot\n",
      "Nearest to another     : last, final, every, olympic, yarn, specific, sheet, simplest\n",
      "Nearest to got         : gets, get, getting, gotten, gave, received, busting, drove\n",
      "Nearest to seems       : seemed, seem, appears, looks, felt, feels, seemingly, would\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    average_loss = 0\n",
    "\n",
    "    gen = batch_generator(data, batch_size=batch_size, num_skips=num_skips, skip_window=skip_window)\n",
    "\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels, sentence = next(gen)\n",
    "        feed_dict = {train_dataset : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "\n",
    "            print \"Average loss at step %d: %0.4f (sentence %d)\" % (step, average_loss, sentence)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % 50000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dict[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                \n",
    "                print \"Nearest to %s:\" % valid_word.ljust(12),\n",
    "                print ', '.join(reverse_dict[nearest[k]] for k in xrange(top_k))\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = ['emotion', 'feeling', 'thriller', 'action', 'horror', 'good', 'comedy', 'bad', 'love']\n",
    "X = final_embeddings[[dictionary[w] for w in words]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>w2v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "      <td>0.479032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>action</td>\n",
       "      <td>horror</td>\n",
       "      <td>0.339351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>horror</td>\n",
       "      <td>thriller</td>\n",
       "      <td>0.331430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emotion</td>\n",
       "      <td>feeling</td>\n",
       "      <td>0.290072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>comedy</td>\n",
       "      <td>thriller</td>\n",
       "      <td>0.287289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>action</td>\n",
       "      <td>thriller</td>\n",
       "      <td>0.276372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>action</td>\n",
       "      <td>comedy</td>\n",
       "      <td>0.174267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>comedy</td>\n",
       "      <td>horror</td>\n",
       "      <td>0.161291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>comedy</td>\n",
       "      <td>good</td>\n",
       "      <td>0.093321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>feeling</td>\n",
       "      <td>thriller</td>\n",
       "      <td>0.080254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>action</td>\n",
       "      <td>emotion</td>\n",
       "      <td>0.063925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>emotion</td>\n",
       "      <td>love</td>\n",
       "      <td>0.059186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>good</td>\n",
       "      <td>love</td>\n",
       "      <td>0.058942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>good</td>\n",
       "      <td>thriller</td>\n",
       "      <td>0.057860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>horror</td>\n",
       "      <td>love</td>\n",
       "      <td>0.056235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>bad</td>\n",
       "      <td>horror</td>\n",
       "      <td>0.052271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emotion</td>\n",
       "      <td>horror</td>\n",
       "      <td>0.046476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>bad</td>\n",
       "      <td>comedy</td>\n",
       "      <td>0.040605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>action</td>\n",
       "      <td>bad</td>\n",
       "      <td>0.034820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>bad</td>\n",
       "      <td>thriller</td>\n",
       "      <td>0.024867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>comedy</td>\n",
       "      <td>feeling</td>\n",
       "      <td>0.016345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>feeling</td>\n",
       "      <td>horror</td>\n",
       "      <td>0.014310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>action</td>\n",
       "      <td>good</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>feeling</td>\n",
       "      <td>love</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>bad</td>\n",
       "      <td>love</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>comedy</td>\n",
       "      <td>emotion</td>\n",
       "      <td>-0.008113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>comedy</td>\n",
       "      <td>love</td>\n",
       "      <td>-0.010402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>action</td>\n",
       "      <td>love</td>\n",
       "      <td>-0.018634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>feeling</td>\n",
       "      <td>good</td>\n",
       "      <td>-0.019525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>action</td>\n",
       "      <td>feeling</td>\n",
       "      <td>-0.042825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>good</td>\n",
       "      <td>horror</td>\n",
       "      <td>-0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>bad</td>\n",
       "      <td>feeling</td>\n",
       "      <td>-0.057497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emotion</td>\n",
       "      <td>thriller</td>\n",
       "      <td>-0.058788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bad</td>\n",
       "      <td>emotion</td>\n",
       "      <td>-0.065494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emotion</td>\n",
       "      <td>good</td>\n",
       "      <td>-0.067005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>love</td>\n",
       "      <td>thriller</td>\n",
       "      <td>-0.093362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2       w2v\n",
       "68      bad      good  0.479032\n",
       "31   action    horror  0.339351\n",
       "38   horror  thriller  0.331430\n",
       "1   emotion   feeling  0.290072\n",
       "56   comedy  thriller  0.287289\n",
       "29   action  thriller  0.276372\n",
       "33   action    comedy  0.174267\n",
       "58   comedy    horror  0.161291\n",
       "59   comedy      good  0.093321\n",
       "11  feeling  thriller  0.080254\n",
       "27   action   emotion  0.063925\n",
       "8   emotion      love  0.059186\n",
       "53     good      love  0.058942\n",
       "47     good  thriller  0.057860\n",
       "44   horror      love  0.056235\n",
       "67      bad    horror  0.052271\n",
       "4   emotion    horror  0.046476\n",
       "69      bad    comedy  0.040605\n",
       "34   action       bad  0.034820\n",
       "65      bad  thriller  0.024867\n",
       "55   comedy   feeling  0.016345\n",
       "13  feeling    horror  0.014310\n",
       "32   action      good  0.004199\n",
       "17  feeling      love  0.002668\n",
       "71      bad      love  0.000074\n",
       "54   comedy   emotion -0.008113\n",
       "62   comedy      love -0.010402\n",
       "35   action      love -0.018634\n",
       "14  feeling      good -0.019525\n",
       "28   action   feeling -0.042825\n",
       "49     good    horror -0.048900\n",
       "64      bad   feeling -0.057497\n",
       "2   emotion  thriller -0.058788\n",
       "63      bad   emotion -0.065494\n",
       "5   emotion      good -0.067005\n",
       "74     love  thriller -0.093362"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_sim = pd.DataFrame(X.dot(X.T), columns=words, index=words)\n",
    "w2v_sim = w2v_sim.unstack().reset_index()\n",
    "w2v_sim = w2v_sim[w2v_sim.level_0 < w2v_sim.level_1]\n",
    "w2v_sim.columns = ['word1', 'word2', 'w2v']\n",
    "w2v_sim.sort_values(by='w2v', ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
