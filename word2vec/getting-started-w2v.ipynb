{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from functools import partial\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=True):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    # Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "   \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def clean_review(review, remove_stopwords=True):\n",
    "    return ' '.join(review_to_wordlist(review, remove_stopwords))\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) == 0:\n",
    "            continue\n",
    "        sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/home/agrigorev/tmp/data/bagofpopcorn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_folder + '/labeledTrainData.tsv', delimiter=\"\\t\", quoting=3)\n",
    "train_unlab = pd.read_csv(data_folder + '/unlabeledTrainData.tsv', delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(data_folder + '/testData.tsv', delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer()\n",
    "stokenizer = partial(review_to_sentences, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigorev/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/agrigorev/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/agrigorev/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "train['review_clean'] = train.review.apply(stokenizer)\n",
    "train_unlab['review_clean'] = train.review.apply(stokenizer)\n",
    "test['review_clean'] = test.review.apply(stokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for ss in train.review_clean:\n",
    "    if isinstance(ss, list):\n",
    "        sentences.extend(ss)\n",
    "\n",
    "for ss in train_unlab.review_clean:\n",
    "    if isinstance(ss, list):\n",
    "        sentences.extend(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count,\n",
    "                 window=context, sample=downsampling, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('man woman child kitchen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.7435693740844727),\n",
       " (u'dreadful', 0.7191959619522095),\n",
       " (u'horrible', 0.6926224231719971),\n",
       " (u'atrocious', 0.689213752746582),\n",
       " (u'abysmal', 0.6871772408485413),\n",
       " (u'lousy', 0.6616935133934021),\n",
       " (u'horrid', 0.6603978872299194),\n",
       " (u'horrendous', 0.6189523935317993),\n",
       " (u'laughable', 0.6157838106155396),\n",
       " (u'appalling', 0.6149518489837646)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('300_features_40_minwords_10context.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74356931"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = 'awful'\n",
    "w2 = 'terrible'\n",
    "v1 = model[w1]\n",
    "v2 = model[w2]\n",
    "v1.dot(v2) / (np.sqrt(v1.dot(v1)) * np.sqrt(v2.dot(v2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From words to paragaphs: \n",
    "\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors\n",
    "- just sum - worse than BoW\n",
    "- weight by TF-IDF - only marginally better\n",
    "- clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13026, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = model.syn0\n",
    "word_count = word_vectors.shape[0]\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = word_count / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms = np.sqrt((word_vectors ** 2).sum(axis=1, keepdims=True))\n",
    "norm_vectors = word_vectors / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrigorev/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.py:1300: RuntimeWarning: init_size=300 should be larger than k=2605. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    }
   ],
   "source": [
    "km = MiniBatchKMeans(init='random', n_clusters=num_clusters, init_size=(3*num_clusters))\n",
    "idx = km.fit_predict(norm_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "\n",
      "[u'attain']\n",
      "\n",
      "Cluster 1\n",
      "\n",
      "[u'perform', u'performing']\n",
      "\n",
      "Cluster 2\n",
      "\n",
      "[u'dumps']\n",
      "\n",
      "Cluster 3\n",
      "\n",
      "[u'wai']\n",
      "\n",
      "Cluster 4\n",
      "\n",
      "[u'jameson']\n",
      "\n",
      "Cluster 5\n",
      "\n",
      "[u'pushed']\n",
      "\n",
      "Cluster 6\n",
      "\n",
      "[u'sterile']\n",
      "\n",
      "Cluster 7\n",
      "\n",
      "[u'ambitious', u'virtue', u'bhandarkar', u'moreover', u'roeg', u'method', u'storyteller', u'input', u'blending', u'flair', u'demonstration', u'expertise', u'professionalism', u'ek', u'uniquely', u'firmly', u'mastery', u'immense', u'skill', u'capable', u'magnetic', u'skills', u'astute', u'distinction', u'benefits', u'excesses', u'attributes', u'sophistication', u'strengths', u'genius', u'grandeur', u'characterized', u'precision', u'restraint', u'knack', u'abilities', u'moods', u'characteristic', u'demonstrates', u'ability', u'capabilities']\n",
      "\n",
      "Cluster 8\n",
      "\n",
      "[u'person', u'everybody', u'anyone', u'someone', u'everyone', u'everything', u'somebody', u'anybody']\n",
      "\n",
      "Cluster 9\n",
      "\n",
      "[u'mourning']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cluster in xrange(0,10):\n",
    "    print \"Cluster %d\" % cluster\n",
    "    print \n",
    "    words = []\n",
    "    for i in xrange(0, len(word_centroid_map.values())):\n",
    "        if word_centroid_map.values()[i] == cluster:\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    cnt = Counter()\n",
    "    cnt.update([word_centroid_map[w] for w in wordlist if w in word_centroid_map])\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concat_lists(lists):\n",
    "    if not isinstance(lists, list):\n",
    "        return []\n",
    "\n",
    "    res = []\n",
    "    for l in lists:\n",
    "        res.extend(l)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boc = partial(create_bag_of_centroids, word_centroid_map=word_centroid_map)\n",
    "train['boc'] = train.review_clean.apply(concat_lists).apply(boc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2514)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = DictVectorizer()\n",
    "X_boc = vect.fit_transform(train.boc)\n",
    "X_boc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest.fit(X_boc, train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['boc'] = test.review_clean.apply(concat_lists).apply(boc)\n",
    "X_boc_test = vect.transform(test.boc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_boc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'id': test['id'], 'sentiment': y_pred})\n",
    "out.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
